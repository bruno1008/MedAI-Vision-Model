{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iib1ZNprAbn7"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Embedding, GRU, Concatenate, BatchNormalization, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "# Montar a drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Caminho para os pesos pré-treinados do CheXNet\n",
        "chexnet_weights = \"/content/drive/MyDrive/Hackathons/MedAIVision/Model/brucechou1983_CheXNet_Keras_0.3.0_weights.h5\"\n",
        "\n",
        "# Função para criar o CheXNet\n",
        "def create_chexnet(chexnet_weights=chexnet_weights, input_size=(224, 224)):\n",
        "    model = tf.keras.applications.DenseNet121(include_top=False, input_shape=input_size + (3,))\n",
        "    x = model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(14, activation=\"sigmoid\", name=\"chexnet_output\")(x)\n",
        "    chexnet = tf.keras.Model(inputs=model.input, outputs=x)\n",
        "    chexnet.load_weights(chexnet_weights)\n",
        "    chexnet = tf.keras.Model(inputs=model.input, outputs=chexnet.layers[-3].output)\n",
        "    return chexnet\n",
        "\n",
        "# Camada do Codificador de Imagens\n",
        "class Image_encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, name=\"image_encoder_block\"):\n",
        "        super().__init__()\n",
        "        self.chexnet = create_chexnet(input_size=(224, 224))\n",
        "        self.chexnet.trainable = False\n",
        "        self.avgpool = tf.keras.layers.AveragePooling2D()\n",
        "\n",
        "    def call(self, data):\n",
        "        op = self.chexnet(data)\n",
        "        op = self.avgpool(op)\n",
        "        op = tf.reshape(op, shape=(-1, op.shape[1] * op.shape[2], op.shape[3]))\n",
        "        return op\n",
        "\n",
        "# Função do Codificador\n",
        "def encoder(image1, image2, dense_dim, dropout_rate):\n",
        "    im_encoder = Image_encoder()\n",
        "    bkfeat1 = im_encoder(image1)\n",
        "    bkfeat2 = im_encoder(image2)\n",
        "    bk_dense = Dense(dense_dim, name='bkdense', activation='relu')\n",
        "    bkfeat1 = bk_dense(bkfeat1)\n",
        "    bkfeat2 = bk_dense(bkfeat2)\n",
        "    concat = Concatenate(axis=1)([bkfeat1, bkfeat2])\n",
        "    bn = BatchNormalization(name=\"encoder_batch_norm\")(concat)\n",
        "    dropout = Dropout(dropout_rate, name=\"encoder_dropout\")(bn)\n",
        "    return dropout\n",
        "\n",
        "# Camada de Atenção Global\n",
        "class Global_Attention(tf.keras.layers.Layer):\n",
        "    def __init__(self, dense_dim):\n",
        "        super().__init__()\n",
        "        self.W1 = Dense(units=dense_dim)\n",
        "        self.W2 = Dense(units=dense_dim)\n",
        "        self.V = Dense(units=1)\n",
        "\n",
        "    def call(self, encoder_output, decoder_h):\n",
        "        decoder_h = tf.expand_dims(decoder_h, axis=1)\n",
        "        tanh_input = self.W1(encoder_output) + self.W2(decoder_h)\n",
        "        tanh_output = tf.nn.tanh(tanh_input)\n",
        "        attention_weights = tf.nn.softmax(self.V(tanh_output), axis=1)\n",
        "        op = attention_weights * encoder_output\n",
        "        context_vector = tf.reduce_sum(op, axis=1)\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "# Camada de Decodificador de Um Passo\n",
        "class One_Step_Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size, embedding_dim, max_pad, dense_dim, name=\"onestepdecoder\"):\n",
        "        super().__init__()\n",
        "        self.dense_dim = dense_dim\n",
        "        self.embedding = Embedding(input_dim=vocab_size + 1, output_dim=embedding_dim, input_length=max_pad, mask_zero=True, name='onestepdecoder_embedding')\n",
        "        self.LSTM = GRU(units=self.dense_dim, return_state=True, name='onestepdecoder_LSTM')\n",
        "        self.attention = Global_Attention(dense_dim=dense_dim)\n",
        "        self.concat = Concatenate(axis=-1)\n",
        "        self.dense = Dense(dense_dim, name='onestepdecoder_embedding_dense', activation='relu')\n",
        "        self.final = Dense(vocab_size + 1, activation='softmax')\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, input_to_decoder, encoder_output, decoder_h):\n",
        "        embedding_op = self.embedding(input_to_decoder)\n",
        "        context_vector, attention_weights = self.attention(encoder_output, decoder_h)\n",
        "        context_vector_time_axis = tf.expand_dims(context_vector, axis=1)\n",
        "        concat_input = self.concat([context_vector_time_axis, embedding_op])\n",
        "        output, decoder_h = self.LSTM(concat_input, initial_state=decoder_h)\n",
        "        output = self.final(output)\n",
        "        return output, decoder_h, attention_weights\n",
        "\n",
        "# Classe do Decodificador Completo\n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, max_pad, embedding_dim, dense_dim, batch_size, vocab_size):\n",
        "        super().__init__()\n",
        "        self.onestepdecoder = One_Step_Decoder(vocab_size=vocab_size, embedding_dim=embedding_dim, max_pad=max_pad, dense_dim=dense_dim)\n",
        "        self.output_array = tf.TensorArray(tf.float32, size=max_pad)\n",
        "        self.max_pad = max_pad\n",
        "        self.batch_size = batch_size\n",
        "        self.dense_dim = dense_dim\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, encoder_output, caption):\n",
        "        decoder_h = tf.zeros_like(encoder_output[:, 0])\n",
        "        output_array = tf.TensorArray(tf.float32, size=self.max_pad)\n",
        "        for timestep in range(self.max_pad):\n",
        "            output, decoder_h, attention_weights = self.onestepdecoder(caption[:, timestep:timestep + 1], encoder_output, decoder_h)\n",
        "            output_array = output_array.write(timestep, output)\n",
        "        output_array = tf.transpose(output_array.stack(), [1, 0, 2])\n",
        "        return output_array\n",
        "\n",
        "# Função para Criar o Modelo\n",
        "def create_model():\n",
        "    input_size = (224, 224)\n",
        "    tokenizer = joblib.load('tokenizer.pkl')\n",
        "    max_pad = 29\n",
        "    batch_size = 100\n",
        "    vocab_size = len(tokenizer.word_index)\n",
        "    embedding_dim = 300\n",
        "    dense_dim = 512\n",
        "    dropout_rate = 0.2\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "    image1 = Input(shape=input_size + (3,))\n",
        "    image2 = Input(shape=input_size + (3,))\n",
        "    caption = Input(shape=(max_pad,))\n",
        "\n",
        "    encoder_output = encoder(image1, image2, dense_dim, dropout_rate)\n",
        "    output = Decoder(max_pad, embedding_dim, dense_dim, batch_size, vocab_size)(encoder_output, caption)\n",
        "    model = tf.keras.Model(inputs=[image1, image2, caption], outputs=output)\n",
        "    model.load_weights('Encoder_Decoder_global_attention.h5')\n",
        "    return model, tokenizer\n",
        "\n",
        "# Função de Predição com Busca por Feixe\n",
        "def beam_search_predict(image1, image2, model, tokenizer, input_size=(224, 224), beam_width=3):\n",
        "    image1 = tf.expand_dims(cv2.resize(image1, input_size, interpolation=cv2.INTER_NEAREST), axis=0)\n",
        "    image2 = tf.expand_dims(cv2.resize(image2, input_size, interpolation=cv2.INTER_NEAREST), axis=0)\n",
        "    image1 = model.get_layer('image_encoder')(image1)\n",
        "    image2 = model.get_layer('image_encoder')(image2)\n",
        "    image1 = model.get_layer('bkdense')(image1)\n",
        "    image2 = model.get_layer('bkdense')(image2)\n",
        "    concat = model.get_layer('concatenate')([image1, image2])\n",
        "    enc_op = model.get_layer('encoder_batch_norm')(concat)\n",
        "    enc_op = model.get_layer('encoder_dropout')(enc_op)\n",
        "\n",
        "    initial_h = tf.zeros_like(enc_op[:, 0])\n",
        "    start_id = tokenizer.texts_to_sequences(['<cls>'])[0][0]\n",
        "    end_id = tokenizer.texts_to_sequences(['<end>'])[0][0]\n",
        "    max_pad = 29\n",
        "\n",
        "    beam = [([start_id], initial_h, 0.0)]  # (sequência, estado oculto, log_prob)\n",
        "    complete_sequences = []\n",
        "\n",
        "    for step in range(max_pad):\n",
        "        new_beam = []\n",
        "        for seq, h, log_prob in beam:\n",
        "            if seq[-1] == end_id:\n",
        "                complete_sequences.append((seq, log_prob))\n",
        "                continue\n",
        "            input_token = np.array([[seq[-1]]])\n",
        "            output, new_h, _ = model.get_layer('decoder').onestepdecoder(input_token, enc_op, h)\n",
        "            probs = tf.nn.softmax(output[0]).numpy()\n",
        "            top_k_indices = np.argsort(probs)[-beam_width:]\n",
        "            top_k_log_probs = np.log(probs[top_k_indices] + 1e-10)\n",
        "            for i in range(beam_width):\n",
        "                word_id = top_k_indices[i]\n",
        "                word_log_prob = top_k_log_probs[i]\n",
        "                new_seq = seq + [word_id]\n",
        "                new_log_prob = log_prob + word_log_prob\n",
        "                if word_id == end_id:\n",
        "                    complete_sequences.append((new_seq, new_log_prob))\n",
        "                else:\n",
        "                    new_beam.append((new_seq, new_h, new_log_prob))\n",
        "        if not new_beam:\n",
        "            break\n",
        "        new_beam = sorted(new_beam, key=lambda x: x[2], reverse=True)[:beam_width]\n",
        "        beam = new_beam\n",
        "\n",
        "    all_sequences = complete_sequences + [(seq, log_prob) for seq, _, log_prob in beam]\n",
        "    if not all_sequences:\n",
        "        return \"<cls>\"\n",
        "    best_seq, _ = max(all_sequences, key=lambda x: x[1])\n",
        "    caption = tokenizer.sequences_to_texts([best_seq])[0]\n",
        "    words = caption.split()\n",
        "    if words[0] == '<cls>':\n",
        "        words = words[1:]\n",
        "    if '<end>' in words:\n",
        "        words = words[:words.index('<end>')]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Função de Predição Simples\n",
        "def predict1(image1, image2=None, model_tokenizer=None):\n",
        "    if image2 is None:\n",
        "        image2 = image1\n",
        "    image1 = cv2.imread(image1, cv2.IMREAD_UNCHANGED)\n",
        "    image2 = cv2.imread(image2, cv2.IMREAD_UNCHANGED)\n",
        "    if image1 is None or image2 is None:\n",
        "        print(\"Deve ser uma imagem\")\n",
        "        return None\n",
        "    if len(image1.shape) == 2:\n",
        "        image1 = cv2.merge([image1, image1, image1])\n",
        "    if len(image2.shape) == 2:\n",
        "        image2 = cv2.merge([image2, image2, image2])\n",
        "    image1 = tf.keras.applications.densenet.preprocess_input(image1)\n",
        "    image2 = tf.keras.applications.densenet.preprocess_input(image2)\n",
        "    if model_tokenizer is None:\n",
        "        model, tokenizer = create_model()\n",
        "    else:\n",
        "        model, tokenizer = model_tokenizer[0], model_tokenizer[1]\n",
        "    predicted_caption = beam_search_predict(image1, image2, model, tokenizer, beam_width=3)\n",
        "    return predicted_caption\n",
        "\n",
        "# Função de Predição com Avaliação BLEU\n",
        "def predict2(true_caption, image1, image2=None, model_tokenizer=None):\n",
        "    if image2 is None:\n",
        "        image2 = image1\n",
        "    image1 = cv2.imread(image1, cv2.IMREAD_UNCHANGED)\n",
        "    image2 = cv2.imread(image2, cv2.IMREAD_UNCHANGED)\n",
        "    if image1 is None or image2 is None:\n",
        "        print(\"Deve ser uma imagem\")\n",
        "        return None\n",
        "    if len(image1.shape) == 2:\n",
        "        image1 = cv2.merge([image1, image1, image1])\n",
        "    if len(image2.shape) == 2:\n",
        "        image2 = cv2.merge([image2, image2, image2])\n",
        "    image1 = tf.keras.applications.densenet.preprocess_input(image1)\n",
        "    image2 = tf.keras.applications.densenet.preprocess_input(image2)\n",
        "    if model_tokenizer is None:\n",
        "        model, tokenizer = create_model()\n",
        "    else:\n",
        "        model, tokenizer = model_tokenizer[0], model_tokenizer[1]\n",
        "    predicted_caption = beam_search_predict(image1, image2, model, tokenizer, beam_width=3)\n",
        "    reference = [true_caption.split()]\n",
        "    prediction = predicted_caption.split()\n",
        "    bleu1 = sentence_bleu(reference, prediction, weights=(1, 0, 0, 0))\n",
        "    bleu2 = sentence_bleu(reference, prediction, weights=(0.5, 0.5, 0, 0))\n",
        "    bleu3 = sentence_bleu(reference, prediction, weights=(0.33, 0.33, 0.33, 0))\n",
        "    bleu4 = sentence_bleu(reference, prediction, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "    return pd.DataFrame([[bleu1, bleu2, bleu3, bleu4]], columns=['bleu1', 'bleu2', 'bleu3', 'bleu4'])\n",
        "\n",
        "# Função para Predição em Lote\n",
        "def function1(image1_list, image2_list, model_tokenizer=None):\n",
        "    if model_tokenizer is None:\n",
        "        model_tokenizer = list(create_model())\n",
        "    predicted_captions = []\n",
        "    for i1, i2 in zip(image1_list, image2_list):\n",
        "        caption = predict1(i1, i2, model_tokenizer)\n",
        "        predicted_captions.append(caption)\n",
        "    return predicted_captions\n",
        "\n",
        "# Função para Predição em Lote com Avaliação BLEU\n",
        "def function2(true_caption_list, image1_list, image2_list):\n",
        "    model_tokenizer = list(create_model())\n",
        "    predicted = pd.DataFrame(columns=['bleu1', 'bleu2', 'bleu3', 'bleu4'])\n",
        "    for c, i1, i2 in zip(true_caption_list, image1_list, image2_list):\n",
        "        caption = predict2(c, i1, i2, model_tokenizer)\n",
        "        predicted = predicted.append(caption, ignore_index=True)\n",
        "    return predicted"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image1_list = [\"img1.jpg\", \"img2.jpg\"]\n",
        "image2_list = [\"img3.jpg\", \"img4.jpg\"]\n",
        "captions = function1(image1_list, image2_list)\n",
        "print(captions)"
      ],
      "metadata": {
        "id": "PQfyb0N3CIG2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}